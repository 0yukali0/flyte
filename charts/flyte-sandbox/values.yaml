# ---------------------------------------------------------------------
# Core System settings
# This section consists of Core components of Flyte and their deployment
# settings. This includes FlyteAdmin service, Datacatalog, FlytePropeller and
# Flyteconsole
# ---------------------------------------------------------------------

flyte:
  #
  # FLYTEADMIN
  #

  flyteadmin: {} # use default config

  #
  # DATACATALOG
  #

  datacatalog: {}

  #
  # FLYTEPROPELLER
  #

  flytepropeller: {}

  #
  # FLYTECONSOLE
  #

  flyteconsole: {}
  #
  # WEBHOOK SETTINGS
  #

  # ----------------------------------------------
  # Sandbox Configuration
  # Sandbox allows to run flyte without any cloud dependencies and can be run even locally on your laptop.
  # This is achieved by replacing cloud service dependencies by k8s local alternatives. These may not be ideal
  # for a high performance setup, but are great to try out flyte
  # -----------------------------------------------
  #
  # REDIS SETTINGS
  #

  redis:
    # --- enable or disable Redis Statefulset installation
    enabled: false

  # ------------------------------------------------
  #
  # COMMON SETTINGS
  #

  common:
    databaseSecret:
      secretManifest:
        apiVersion: v1
        data:
          pass.txt: YXdlc29tZXNhdWNl
        kind: Secret
        metadata:
          name: "db-pass"
          namespace: flyte
        type: Opaque
    ingress:
      # enable HMR route to flyteconsole for frontend development.
      webpackHMR: true
      tls:
        enabled: false
    flyteNamespaceTemplate: {}


  # -----------------------------------------------------
  # Core dependencies that should be configured for Flyte to work on any platform
  # Specifically 2 - Storage (s3, gcs etc), Production RDBMS - Aurora, CloudSQL etc
  # ------------------------------------------------------
  #
  # STORAGE SETTINGS
  #

  storage:
    # -- Sets the storage type. Supported values are sandbox, s3, gcs and custom.
    type: sandbox
    # -- bucketName defines the storage bucket flyte will use. Required for all types except for sandbox.
    bucketName: my-s3-bucket

  # Database configuration
  db:
    database:
      port: 5432
      username: postgres
      host: postgres
      dbname: postgres

  # --------------------------------------------------------------------
  # Specializing your deployment using configuration
  # -------------------------------------------------------------------
  #
  # CONFIGMAPS SETTINGS
  #

  configmap:
    # Task default resources configuration
    task_resource_defaults:
      task_resources:
        defaults:
          memory: 200Mi

    # Core propeller configuration
    core:
      propeller:
        rawoutput-prefix: s3://my-s3-bucket/
        workers: 20
        max-workflow-retries: 30

    # Plugins configuration
    enabled_plugins:
      tasks:
        task-plugins:
          enabled-plugins:
            - container
            - sidecar
            - k8s-array
          default-for-task-types:
            container: container
            sidecar: sidecar
            container_array: k8s-array

    # Kubernetes plugin configuration
    k8s:
      plugins:
        k8s:
          default-env-vars:
            - FLYTE_AWS_ENDPOINT: "http://minio.flyte:9000"
            - FLYTE_AWS_ACCESS_KEY_ID: minio
            - FLYTE_AWS_SECRET_ACCESS_KEY: miniostorage
          default-memory: 200Mi

    # Logger configuration
    logger:
      logger:
        show-source: true
        level: 4

    # Resource manager configuration
    resource_manager:
      propeller:
        resourcemanager:
          type: noop
          redis: null

    # Tasks logs plugin configuration
    task_logs:
      plugins:
        logs:
          kubernetes-enabled: true
          kubernetes-template-uri: "http://localhost:30082/#/log/{{ .namespace }}/{{ .podName }}/pod?namespace={{ .namespace }}"
    # ----------------------------------------------------------------
    # Optional Modules
    # Flyte built extensions that enable various additional features in Flyte.
    # All these features are optional, but are critical to run certain features
    # ------------------------------------------------------------------------

    # -- **Optional Component**
    # Flyte uses a cloud hosted Cron scheduler to run workflows on a schedule. The following module is optional. Without,
    # this module, you will not have scheduled launchplans / workflows.
    # Docs: https://docs.flyte.org/en/latest/howto/enable_and_use_schedules.html#setting-up-scheduled-workflows
    workflow_scheduler:
      enabled: false
      config: {}

    # -- **Optional Component**
    # Workflow notifications module is an optional dependency. Flyte uses cloud native pub-sub systems to notify users of
    # various events in their workflows
    workflow_notifications:
      enabled: false
      config: {}

    # -- Configuration for the Cluster resource manager component. This is an optional component, that enables automatic
    # cluster configuration. This is useful to set default quotas, manage namespaces etc that map to a project/domain
    cluster_resource_manager:
      # -- Enables the Cluster resource manager component
      enabled: true

    # --------------------------------------------------------
    # Optional Plugins
    # --------------------------------------------------------

    # -- Optional: Spark Plugin using the Spark Operator
    sparkoperator:
      # --- enable or disable Sparkoperator deployment installation
      enabled: false

      # -- Spark plugin configuration
      plugin_config:
        plugins:
          spark:
            # -- Spark default configuration
            spark-config-default:
              # We override the default credentials chain provider for Hadoop so that
              # it can use the serviceAccount based IAM role or ec2 metadata based.
              # This is more in line with how AWS works
              - spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
              - spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: "2"
              - spark.kubernetes.allocation.batch.size: "50"
              - spark.hadoop.fs.s3a.acl.default: "BucketOwnerFullControl"
              - spark.hadoop.fs.s3n.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
              - spark.hadoop.fs.AbstractFileSystem.s3n.impl: "org.apache.hadoop.fs.s3a.S3A"
              - spark.hadoop.fs.s3.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
              - spark.hadoop.fs.AbstractFileSystem.s3.impl: "org.apache.hadoop.fs.s3a.S3A"
              - spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
              - spark.hadoop.fs.AbstractFileSystem.s3a.impl: "org.apache.hadoop.fs.s3a.S3A"
              - spark.hadoop.fs.s3a.multipart.threshold: "536870912"
              - spark.blacklist.enabled: "true"
              - spark.blacklist.timeout: "5m"
              - spark.task.maxfailures: "8"

    # -----------------
    # -- Training on AWS Sagemaker using AWS Sagemaker operator. To actually install the operator, please follow instructions [here](https://github.com/aws/amazon-sagemaker-operator-for-k8s/tree/master/hack/charts/installer/rolebased)
    # Use the config section here to just enable sagemaker plugin in Flyte, after you have installed the operator using the information
    sagemaker:
      enabled: false
      plugin_config:
        plugins:
          sagemaker:
            roleArn: <arn>
            region: <region>
# ---------------------------
# -- Flink jobs using the Flink Operator: this is work in progress

#
# POSTGRES SETTINGS
#

postgres:
  # --- enable or disable Postgres deployment installation
  enabled: true
  # -- Replicas count for Postgres deployment
  replicaCount: 1
  image:
    # -- Docker image for Postgres deployment
    repository: postgres
    tag: "10.16-alpine"
    pullPolicy: IfNotPresent
  # -- Default resources requests and limits for Postgres deployment
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 1000m
      memory: 512Mi
  # -- Service settings for Postgres
  service:
    annotations: {}
    type: ClusterIP
  # -- Annotations for Postgres pods
  podAnnotations: {}
  # -- nodeSelector for Postgres deployment
  nodeSelector: {}
  # -- tolerations for Postgres deployment
  tolerations: []
  # -- affinity for Postgres deployment
  affinity: {}

#
# MINIO SETTINGS
#

minio:
  # --- enable or disable Minio deployment installation
  enabled: true
  # -- Replicas count for Minio deployment
  replicaCount: 1
  image:
    # -- Docker image for Minio deployment
    repository: minio/minio
    tag: RELEASE.2020-12-16T05-05-17Z
    pullPolicy: IfNotPresent
  # -- Default resources requests and limits for Minio deployment
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 512Mi
  # -- Service settings for Minio
  service:
    annotations: {}
    type: ClusterIP
  # -- Annotations for Minio pods
  podAnnotations: {}
  # -- nodeSelector for Minio deployment
  nodeSelector: {}
  # -- tolerations for Minio deployment
  tolerations: []
  # -- affinity for Minio deployment
  affinity: {}

#
# CONTOUR SETTINGS
#

contour:
  # --- enable or disable Contour deployment installation
  enabled: true
  # -- Replicas count for Contour deployment
  replicaCount: 1
  contour:
    # -- Default resources requests and limits for Contour
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
      limits:
        cpu: 100m
        memory: 100Mi
  envoy:
    # -- Default resources requests and limits for Envoy
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
      limits:
        cpu: 100m
        memory: 100Mi
    service:
      type: NodePort
      ports:
        http: 80
      nodePorts:
        http: 30081
  # -- Annotations for ServiceAccount attached to Contour pods
  serviceAccountAnnotations: {}
  # -- Annotations for Contour pods
  podAnnotations: {}
  # -- nodeSelector for Contour deployment
  nodeSelector: {}
  # -- tolerations for Contour deployment
  tolerations: []
  # -- affinity for Contour deployment

#
# KUBERNETES DASHBOARD
#

kubernetes-dashboard:
  enabled: true
  extraArgs:
    - --enable-skip-login
    - --enable-insecure-login
    - --disable-settings-authorizer
  protocolHttp: true
  service:
    type: NodePort
    externalPort: 30082

# -- Optional: Spark Plugin using the Spark Operator
sparkoperator:
  # --- enable or disable Sparkoperator deployment installation
  enabled: false
  # -- Replicas count for Sparkoperator deployment
  replicaCount: 1
  image:
    # -- Docker image for Sparkoperator
    tag: v1beta2-1.2.0-3.0.0 # Set to v1beta2-1.1.2-2.4.5 for Spark 2
  # -- Default resources requests and limits for Sparkoperator
  resources:
    limits:
      cpu: 1000m
      memory: 500M
    requests:
      cpu: 10m
      memory: 50M

pytorchoperator:
  # --- enable or disable Pytorchoperator deployment installation
  enabled: false # Set false to disable
  # -- Replicas count for Pytorchoperator deployment
  replicaCount: 1
  image:
    # -- Docker image for Pytorchoperator
    repository: gcr.io/kubeflow-images-public/pytorch-operator
    tag: v1.0.0-g047cf0f
    pullPolicy: IfNotPresent
  # -- Default resources requests and limits for Pytorchoperator
  resources:
    limits:
      cpu: 500m
      memory: 1000M
    requests:
      cpu: 10m
      memory: 50M
  # -- Service settings for Pytorchoperator
  service:
    annotations: {}
    type: ClusterIP
  # -- Annotations for ServiceAccount attached to Pytorchoperator pods
  serviceAccountAnnotations: {}
  # -- Annotations for Pytorchoperator pods
  podAnnotations: {}
  # -- nodeSelector for Pytorchoperator deployment
  nodeSelector: {}
  # -- tolerations for Pytorchoperator deployment
  tolerations: []
  # -- affinity for Pytorchoperator deployment
  affinity: {}

# ----------------------
# -- Distributed tensorflow training using the TF-Operator. This can be installed as explained [here](https://github.com/kubeflow/tf-operator)
# There is no helm chart maintained and hence it might need an independent installation
tf_operator:
  enabled: false